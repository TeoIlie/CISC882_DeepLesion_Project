
@article{deeplesion,
	title = {{DeepLesion}: automated mining of large-scale lesion annotations and universal lesion detection with deep learning},
	volume = {5},
	issn = {2329-4302, 2329-4310},
	shorttitle = {{DeepLesion}},
	url = {https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-3/036501/DeepLesion--automated-mining-of-large-scale-lesion-annotations-and/10.1117/1.JMI.5.3.036501.full},
	doi = {10.1117/1.JMI.5.3.036501},
	abstract = {Extracting, harvesting, and building large-scale annotated radiological image datasets is a greatly important yet challenging problem. Meanwhile, vast amounts of clinical annotations have been collected and stored in hospitals’ picture archiving and communication systems (PACS). These types of annotations, also known as bookmarks in PACS, are usually marked by radiologists during their daily workflow to highlight significant image findings that may serve as reference for later studies. We propose to mine and harvest these abundant retrospective medical data to build a large-scale lesion image dataset. Our process is scalable and requires minimum manual annotation effort. We mine bookmarks in our institute to develop DeepLesion, a dataset with 32,735 lesions in 32,120 CT slices from 10,594 studies of 4,427 unique patients. There are a variety of lesion types in this dataset, such as lung nodules, liver tumors, enlarged lymph nodes, and so on. It has the potential to be used in various medical image applications. Using DeepLesion, we train a universal lesion detector that can find all types of lesions with one unified framework. In this challenging task, the proposed lesion detector achieves a sensitivity of 81.1\% with five false positives per image.},
	number = {3},
	urldate = {2024-11-12},
	journal = {Journal of Medical Imaging},
	author = {Yan, Ke and Wang, Xiaosong and Lu, Le and Summers, Ronald M.},
	month = jul,
	year = {2018},
	note = {Publisher: SPIE},
	pages = {036501},
	file = {Full Text PDF:/Users/Teo/Zotero/storage/NJGDH8HL/Yan et al. - 2018 - DeepLesion automated mining of large-scale lesion annotations and universal lesion detection with d.pdf:application/pdf},
}

@misc{cancer,
	title = {The {Global} {Cancer} {Burden} {\textbar} {American} {Cancer} {Society}},
	url = {https://www.cancer.org/about-us/our-global-health-work/global-cancer-burden.html#},
}

@article{workload,
	title = {Mandating {Limits} on {Workload}, {Duty}, and {Speed} in                     {Radiology}},
	volume = {304},
	issn = {0033-8419},
	url = {https://pubs.rsna.org/doi/full/10.1148/radiol.212631},
	doi = {10.1148/radiol.212631},
	abstract = {Research has not yet quantified the effects of workload or duty hours on the accuracy of radiologists. With the exception of a brief reduction in imaging studies during the 2020 peak of the COVID-19 pandemic, the workload of radiologists in the United States has seen relentless growth in recent years. One concern is that this increased demand could lead to reduced accuracy. Behavioral studies in species ranging from insects to humans have shown that decision speed is inversely correlated to decision accuracy. A potential solution is to institute workload and duty limits to optimize radiologist performance and patient safety. The concern, however, is that any prescribed mandated limits would be arbitrary and thus no more advantageous than allowing radiologists to self-regulate. Specific studies have been proposed to determine whether limits reduce error, and if so, to provide a principled basis for such limits. This could determine the precise susceptibility of individual radiologists to medical error as a function of speed during image viewing, the maximum number of studies that could be read during a work shift, and the appropriate shift duration as a function of time of day. Before principled recommendations for restrictions are made, however, it is important to understand how radiologists function both optimally and at the margins of adequate performance. This study examines the relationship between interpretation speed and error rates in radiology, the potential influence of artificial intelligence on reading speed and error rates, and the possible outcomes of imposed limits on both caseload and duty hours. This review concludes that the scientific evidence needed to make meaningful rules is lacking and notes that regulating workloads without scientific principles can be more harmful than not regulating at all.

© RSNA, 2022},
	number = {2},
	urldate = {2024-11-12},
	journal = {Radiology},
	author = {Alexander, Robert and Waite, Stephen and Bruno, Michael A. and Krupinski, Elizabeth A. and Berlin, Leonard and Macknik, Stephen and Martinez-Conde, Susana},
	month = aug,
	year = {2022},
	note = {Publisher: Radiological Society of North America},
	pages = {274--282},
	file = {Full Text PDF:/Users/Teo/Zotero/storage/LFWXWNB8/Alexander et al. - 2022 - Mandating Limits on Workload, Duty, and Speed in                     Radiology.pdf:application/pdf},
}

@article{oncology,
	title = {{PET}/{CT} in oncology},
	volume = {12},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC4952129/},
	doi = {10.7861/clinmedicine.12-4-368},
	language = {en},
	number = {4},
	urldate = {2024-11-13},
	journal = {Clinical Medicine},
	author = {Ul-Hassan, Fahim and Cook, Gary J.},
	month = aug,
	year = {2012},
	pmid = {22930885},
	pages = {368},
	file = {Full Text PDF:/Users/Teo/Zotero/storage/BA5QWFTM/Ul-Hassan and Cook - 2012 - PETCT in oncology.pdf:application/pdf},
}

@book {PMID:33620865,
	Title = {CT Scan},
	Author = {Patel, Paula R. and De Jesus, Orlando},
	Abstract = {A computed tomography (CT) scan, commonly referred to as a CT, is a radiological imaging study. The machine was developed by physicist Allan MacLeod Cormack and electrical engineer Godfrey Hounsfield. Their development awarded them the Nobel prize in Physiology or Medicine in 1979. The first scanners were installed in 1974. Since then, technological advances and math have allowed single images to be computed into two-dimensional informative images. The CT scan is essentially an X-ray study, where a series of rays are rotated around a specified body part, and computer-generated cross-sectional images are produced. The advantage of these tomographic images compared to conventional X-rays is that they contain detailed information of a specified area in cross-section, eliminating the superimposition of images, which provides a tremendous advantage over plain films. CT scans provide excellent clinicopathological correlation for a suspected illness.  The use of CT scans augments the physician's ability to diagnose a patient's illness accurately. Low-dose CT scans are proving useful in preventative medicine and cancer screening. The study was initially called a CAT scan representing computer axial tomography, where the table moved after each axial image was obtained.  In a spiral or helical scan, the table moves continuously as the x-ray source and detectors rotate. This reduces the duration of the study significantly to provide quick results in emergent situations. It rapidly substituted cerebral angiography for detecting head trauma injuries and brain masses in a fast and extremely reliable way. A radiologic technician acquires CT scans, which are interpreted and reported by a trained radiologist.},
	Publisher = {StatPearls Publishing, Treasure Island (FL)},
	Year = {2023},
	URL = {http://europepmc.org/books/NBK567796},
}

@article{radiology_error,
	title = {Interpretive {Error} in {Radiology}},
	volume = {208},
	issn = {0361-803X},
	url = {https://www.ajronline.org/doi/10.2214/AJR.16.16963},
	doi = {10.2214/AJR.16.16963},
	abstract = {OBJECTIVE. Although imaging technology has advanced significantly since the work of Garland in 1949, interpretive error rates remain unchanged. In addition to patient harm, interpretive errors are a major cause of litigation and distress to radiologists. In this article, we discuss the mechanics involved in searching an image, categorize omission errors, and discuss factors influencing diagnostic accuracy. Potential individual- and system-based solutions to mitigate or eliminate errors are also discussed.
CONCLUSION. Radiologists use visual detection, pattern recognition, memory, and cognitive reasoning to synthesize final interpretations of radiologic studies. This synthesis is performed in an environment in which there are numerous extrinsic distractors, increasing workloads and fatigue. Given the ultimately human task of perception, some degree of error is likely inevitable even with experienced observers. However, an understanding of the causes of interpretive errors can help in the development of tools to mitigate errors and improve patient safety.},
	number = {4},
	urldate = {2024-11-14},
	journal = {American Journal of Roentgenology},
	author = {Waite, Stephen and Scott, Jinel and Gale, Brian and Fuchs, Travis and Kolla, Srinivas and Reede, Deborah},
	month = apr,
	year = {2017},
	note = {Publisher: American Roentgen Ray Society},
	pages = {739--749},
	file = {Full Text PDF:/Users/Teo/Zotero/storage/3LKXLCY6/Waite et al. - 2017 - Interpretive Error in Radiology.pdf:application/pdf},
}

@article{perception_error,
	title = {Significant on-call misses by radiology residents interpreting computed tomographic studies: {Perception} versus cognition},
	volume = {4},
	issn = {1438-1435},
	shorttitle = {Significant on-call misses by radiology residents interpreting computed tomographic studies},
	url = {https://doi.org/10.1007/BF01461735},
	doi = {10.1007/BF01461735},
	abstract = {The purpose of this study was to evaluate the etiology of significant false-negative computed tomographic (CT) interpretations by radiology residents on-call. Over a 1-year period, significant on-call false-negative CT interpretations were analyzed to determine whether errors were perceptual (i.e., the resident did not see the finding or findings) or cognitive (i.e., the resident did not recognize the implications or misinterpreted a finding or findings). Significant “misses” were defined as errors that delayed surgical treatment or misdirected management in a potentially life-threatening manner.},
	language = {en},
	number = {5},
	urldate = {2024-11-14},
	journal = {Emergency Radiology},
	author = {Funaki, Brian and Szymski, George X. and Rosenblum, Jordan D.},
	month = sep,
	year = {1997},
	keywords = {Diagnostic radiology, Medical Imaging, Observer performance, Radiology education},
	pages = {290--294},
	file = {Full Text PDF:/Users/Teo/Zotero/storage/AUYVUP72/Funaki et al. - 1997 - Significant on-call misses by radiology residents interpreting computed tomographic studies Percept.pdf:application/pdf},
}

@article{consensus,
    author = {Khalifa, Mohamed and Albadawy, Mona},
    year = {2024},
    month = {03},
    pages = {100146},
    title = {AI in Diagnostic Imaging: Revolutionising Accuracy and Efficiency},
    volume = {5},
    journal = {Computer Methods and Programs in Biomedicine Update},
    doi = {10.1016/j.cmpbup.2024.100146}
}


@article{medical_error,
	title = {Medical error—the third leading cause of death in the {US}},
	volume = {353},
	copyright = {Published by the BMJ Publishing Group Limited. For permission to use (where not already granted under a licence) please go to http://group.bmj.com/group/rights-licensing/permissions},
	issn = {1756-1833},
	url = {https://www.bmj.com/content/353/bmj.i2139},
	doi = {10.1136/bmj.i2139},
	abstract = {{\textless}p{\textgreater}Medical error is not included on death certificates or in rankings of cause of death. \textbf{Martin Makary} and \textbf{Michael Daniel} assess its contribution to mortality and call for better reporting {\textless}/p{\textgreater}},
	language = {en},
	urldate = {2024-11-14},
	journal = {BMJ},
	author = {Makary, Martin A. and Daniel, Michael},
	month = may,
	year = {2016},
	pmid = {27143499},
	note = {Publisher: British Medical Journal Publishing Group
Section: Analysis},
	pages = {i2139},
	file = {Full Text PDF:/Users/Teo/Zotero/storage/IH23S8EE/Makary and Daniel - 2016 - Medical error—the third leading cause of death in the US.pdf:application/pdf},
}


@article{ai_foe,
	title = {Artificial {Intelligence} for {Image} {Interpretation}: {Counterpoint}—{The} {Radiologist}'s {Incremental} {Foe}},
	volume = {217},
	issn = {0361-803X},
	shorttitle = {Artificial {Intelligence} for {Image} {Interpretation}},
	url = {https://www.ajronline.org/doi/10.2214/AJR.21.25484},
	doi = {10.2214/AJR.21.25484},
	number = {3},
	urldate = {2024-11-14},
	journal = {American Journal of Roentgenology},
	author = {Lexa, Frank J. and Jha, Saurabh},
	month = sep,
	year = {2021},
	note = {Publisher: American Roentgen Ray Society},
	pages = {558--559},
	file = {Full Text PDF:/Users/Teo/Zotero/storage/3J79LV8U/Lexa and Jha - 2021 - Artificial Intelligence for Image Interpretation Counterpoint—The Radiologist's Incremental Foe.pdf:application/pdf},
}


@article{doctors_ftw,
	title = {{AI} {Will} {Change} {Radiology}, but {It} {Won}’t {Replace} {Radiologists}},
	issn = {0017-8012},
	url = {https://hbr.org/2018/03/ai-will-change-radiology-but-it-wont-replace-radiologists},
	abstract = {Recent advances in artificial intelligence have led to speculation that AI might one day replace human radiologists. Researchers have developed deep learning neural networks that can identify pathologies in radiological images such as bone fractures and potentially cancerous lesions, in some cases more reliably than an average radiologist. But the great majority of radiologists will continue to have jobs in the decades to come — jobs that will be altered and enhanced by AI. Because of this, they will need to adopt new skills and work processes. The only radiologists whose jobs may be threatened are the ones who refuse to work with AI.},
	urldate = {2024-11-14},
	journal = {Harvard Business Review},
	author = {Davenport, Thomas H. and Keith J. Dreyer, D. O.},
	month = mar,
	year = {2018},
	note = {Section: Innovation},
	keywords = {Healthcare sector, Innovation, Technology and analytics},
	file = {Snapshot:/Users/Teo/Zotero/storage/L2QAYD4M/ai-will-change-radiology-but-it-wont-replace-radiologists.html:text/html},
}

@article{ai_adoption,
	title = {2023 {Industry} {Perceptions} {Survey} on {AI} {Adoption} and {Return} on {Investment}},
	issn = {2948-2933},
	url = {https://doi.org/10.1007/s10278-024-01147-1},
	doi = {10.1007/s10278-024-01147-1},
	abstract = {This SIIM-sponsored 2023 report highlights an industry view on artificial intelligence adoption barriers and success related to diagnostic imaging, life sciences, and contrasts. In general, our 2023 survey indicates that there has been progress in adopting AI across multiple uses, and there continues to be an optimistic forecast for the impact on workflow and clinical outcomes. This report, as in prior years, should be seen as a snapshot of the use of AI in imaging. Compared to our 2021 survey, the 2023 respondents expressed wider AI adoption but felt this was behind the potential. Specifically, the adoption has increased as sources of return on investment with AI in radiology are better understood as documented by vendor/client use case studies. Generally, the discussions of AI solutions centered on workflow triage, visualization, detection, and characterization. Generative AI was also mentioned for improving productivity in reporting. As payor reimbursement remains elusive, the ROI discussions expanded to look at other factors, including increased hospital procedures and admissions, enhanced radiologist productivity for practices, and improved patient outcomes for integrated health networks. When looking at the longer-term horizon for AI adoption, respondents frequently mentioned that the opportunity for AI to achieve greater adoption with more complex AI and a more manageable/visible ROI is outside the USA. Respondents focused on the barriers to trust in AI and the FDA processes.},
	language = {en},
	urldate = {2024-11-14},
	journal = {Journal of Imaging Informatics in Medicine},
	author = {Goldburgh, Mitchell and LaChance, Michael and Komissarchik, Julia and Patriarche, Julia and Chapa, Joe and Chen, Oliver and Deshpande, Priya and Geeslin, Matthew and Komissarchik, Julia and Kottler, Nina and Patriarche, Julia and Sommer, Jennifer and Ayers, Marcus and Vujic, Vedrana},
	month = aug,
	year = {2024},
	keywords = {AI Adoption, Artificial Intelligence, Market Survey},
	file = {Full Text PDF:/Users/Teo/Zotero/storage/NCJ2DUUV/Goldburgh et al. - 2024 - 2023 Industry Perceptions Survey on AI Adoption and Return on Investment.pdf:application/pdf},
}

@article{deep_learning_survey,
    title = {A survey on deep learning in medical image analysis},
    journal = {Medical Image Analysis},
    volume = {42},
    pages = {60-88},
    year = {2017},
    issn = {1361-8415},
    doi = {https://doi.org/10.1016/j.media.2017.07.005},
    url = {https://www.sciencedirect.com/science/article/pii/S1361841517301135},
    author = {Geert Litjens and Thijs Kooi and Babak Ehteshami Bejnordi and Arnaud Arindra Adiyoso Setio and Francesco Ciompi and Mohsen Ghafoorian and Jeroen A.W.M. {van der Laak} and Bram {van Ginneken} and Clara I. Sánchez},
    keywords = {Deep learning, Convolutional neural networks, Medical imaging, Survey},
    abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.}
}


@book{chollet_deep_learning,
	title = {Deep {Learning} with {Python}},
	url = {https://www.manning.com/books/deep-learning-with-python},
	urldate = {2024-11-14},
	publisher = {Manning},
	author = {Chollet, François},
	month = nov,
	year = {2017},
	file = {Deep Learning with Python:/Users/Teo/Zotero/storage/PPY5952K/deep-learning-with-python.html:text/html},
}

@article{yolo,
title = {A Review of Yolo Algorithm Developments},
journal = {Procedia Computer Science},
volume = {199},
pages = {1066-1073},
year = {2022},
note = {The 8th International Conference on Information Technology and Quantitative Management (ITQM 2020 \& 2021): Developing Global Digital Economy after COVID-19},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.135},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922001363},
author = {Peiyuan Jiang and Daji Ergu and Fangyao Liu and Ying Cai and Bo Ma},
keywords = {Review, Yolo, Object Detection, Public Data Analysis},
abstract = {Object detection techniques are the foundation for the artificial intelligence field. This research paper gives a brief overview of the You Only Look Once (YOLO) algorithm and its subsequent advanced versions. Through the analysis, we reach many remarks and insightful results. The results show the differences and similarities among the YOLO versions and between YOLO and Convolutional Neural Networks (CNNs). The central insight is the YOLO algorithm improvement is still ongoing.This article briefly describes the development process of the YOLO algorithm, summarizes the methods of target recognition and feature selection, and provides literature support for the targeted picture news and feature extraction in the financial and other fields. Besides, this paper contributes a lot to YOLO and other object detection literature.}
}

@inproceedings{cnn_lecun_bengio,
  title={Convolutional networks for images, speech, and time series},
  author={Yann LeCun and Yoshua Bengio},
  year={1998},
  url={https://api.semanticscholar.org/CorpusID:6916627}
}

@article{yolov3,
  title={YOLOv3: An Incremental Improvement},
  author={Joseph Redmon and Ali Farhadi},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.02767},
  url={https://api.semanticscholar.org/CorpusID:4714433}
}


@misc{kaggle,
	title = {{DeepLesion} {Overview}},
	url = {https://kaggle.com/code/kmader/deeplesion-overview},
	abstract = {Explore and run machine learning code with Kaggle Notebooks {\textbar} Using data from NIH DeepLesion Subset},
	language = {en},
	urldate = {2024-12-06},
	file = {Snapshot:/Users/Teo/Zotero/storage/2ZYH4ZI3/deeplesion-overview.html:text/html},
}


@misc{unet,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2024-12-06},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: conditionally accepted at MICCAI 2015},
	file = {Preprint PDF:/Users/Teo/Zotero/storage/R66S96A4/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image Segmentation.pdf:application/pdf;Snapshot:/Users/Teo/Zotero/storage/RLHS8EMA/1505.html:text/html},
}

@misc{Tamhane_Saxena_Huang, title={A deep learning framework for detecting lesions in CT scans from deep lesion dataset}, url={https://github.com/anir16293/Deep-Lesion}, journal={GitHub}, author={Tamhane, Aniruddha and Saxena, Parv and Huang, Wei-Lun}} 